{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN12oe0pRc2OQB32+eEreKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvir0188/plant-and-crop-pest-detection/blob/main/cse475_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-cb-G6lkBYS",
        "outputId": "ffbdab26-299a-43d9-c2d5-b83126748655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras"
      ],
      "metadata": {
        "id": "HEwqTVkokRG5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FabeK-pHk_oF",
        "outputId": "0c57b2cb-29cf-4e30-a91e-cbf65ca05bb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import keras\n",
        "from keras.layers import LeakyReLU\n",
        "# splitfolders needs to be installed with ! since it's a shell command\n",
        "!pip install split-folders\n",
        "import splitfolders\n",
        "from keras.layers import*\n",
        "from keras.models import*\n",
        "from keras.preprocessing import image\n",
        "# Try using tf.keras.preprocessing.image.ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4YwnNBJlibD",
        "outputId": "a5aa60ce-d1df-4d68-d440-704d55453ae5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.10/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "yrCbn7kclsCe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "path = '/content/drive/MyDrive/cropdata'"
      ],
      "metadata": {
        "id": "j5_GQb0TlwW5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitfolders.ratio(path,output=\"/content/drive/MyDrive/datab\",seed=58, ratio=(.8, .1,.1), group_prefix=None, move=False)"
      ],
      "metadata": {
        "id": "jFO8hFndosmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "train=ImageDataGenerator(rescale=1/255)\n",
        "val=ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Load datasets\n",
        "train_dataset=train.flow_from_directory('/content/drive/MyDrive/datab/train',target_size=(250,250), batch_size=64,class_mode='categorical')\n",
        "validation_dataset = val.flow_from_directory('/content/drive/MyDrive/datab/val', target_size=(250,250), batch_size=32,class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR7OW7BpoRgq",
        "outputId": "ee413269-8050-45e4-c27f-d3310864cbdc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20168 images belonging to 22 classes.\n",
            "Found 2511 images belonging to 22 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.class_indices\n",
        "validation_dataset.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCWWAoxtoUZU",
        "outputId": "3da9adca-e062-40fd-edf1-96c9daf8d9a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cashew anthracnose': 0,\n",
              " 'Cashew gumosis': 1,\n",
              " 'Cashew healthy': 2,\n",
              " 'Cashew leaf miner': 3,\n",
              " 'Cashew red rust': 4,\n",
              " 'Cassava bacterial blight': 5,\n",
              " 'Cassava brown spot': 6,\n",
              " 'Cassava green mite': 7,\n",
              " 'Cassava healthy': 8,\n",
              " 'Cassava mosaic': 9,\n",
              " 'Maize fall armyworm': 10,\n",
              " 'Maize grasshoper': 11,\n",
              " 'Maize healthy': 12,\n",
              " 'Maize leaf beetle': 13,\n",
              " 'Maize leaf blight': 14,\n",
              " 'Maize leaf spot': 15,\n",
              " 'Maize streak virus': 16,\n",
              " 'Tomato healthy': 17,\n",
              " 'Tomato leaf blight': 18,\n",
              " 'Tomato leaf curl': 19,\n",
              " 'Tomato septoria leaf spot': 20,\n",
              " 'Tomato verticulium wilt': 21}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu', input_shape=(250, 250, 3)))\n",
        "model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))\n",
        "model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(22, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer,metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
      ],
      "metadata": {
        "id": "66I7aTQ8ouSy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(train_dataset, validation_data= validation_dataset,epochs=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h2dkIrosr-pQ",
        "outputId": "6a95f4c7-5bbf-488a-91f5-c7a7a6aad2f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m 10/316\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46:55\u001b[0m 33s/step - accuracy: 0.0786 - loss: 3.0762 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nOSError: broken data stream when reading image file\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 260, in _get_iterator\n    for i, batch in enumerate(gen_fn()):\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 253, in generator_fn\n    yield self.py_dataset[i]\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\", line 292, in load_img\n    img = img.resize(width_height_tuple, resample)\n\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2293, in resize\n    self.load()\n\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 312, in load\n    raise _get_oserror(err_code, encoder=False)\n\nOSError: broken data stream when reading image file\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_6064]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7e4fd9b1bc6b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nOSError: broken data stream when reading image file\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 260, in _get_iterator\n    for i, batch in enumerate(gen_fn()):\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 253, in generator_fn\n    yield self.py_dataset[i]\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\", line 292, in load_img\n    img = img.resize(width_height_tuple, resample)\n\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2293, in resize\n    self.load()\n\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 312, in load\n    raise _get_oserror(err_code, encoder=False)\n\nOSError: broken data stream when reading image file\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_6064]"
          ]
        }
      ]
    }
  ]
}